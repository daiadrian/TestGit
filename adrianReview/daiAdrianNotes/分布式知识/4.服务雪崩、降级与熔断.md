## 服务雪崩、降级与熔断

### 服务雪崩

![服务雪崩1](.\images\服务雪崩1.png)

​		`Service A`的流量波动很大，流量经常会突然性增加！那么在这种情况下，就算`Service A`能扛得住请求，`Service B`和`Service C`未必能扛得住这突发的请求。
​		此时，如果`Service C`因为抗不住请求，变得不可用。那么`Service B`的请求也会阻塞，慢慢耗尽`Service B`的线程资源，`Service B`就会变得不可用。紧接着，`Service A`也会不可用，这个时候**整个调用链的服务都会变成失败的状态**，这种情况就是**<font color=red>服务雪崩</font>**

#### 服务雪崩产生的原因

1. 服务提供者不可用

   - 硬件故障

     > 硬件故障可能为硬件损坏造成的服务器主机宕机, 网络硬件故障造成的服务提供者的不可访问

   - 程序Bug

   - 缓存击穿

     > 1. 缓存击穿一般发生在缓存应用重启，所有缓存被清空时
     >
     > 2. 短时间内大量缓存失效时，大量的缓存不命中，使请求直击后端,造成服务提供者超负荷运行，引起服务不可用.

   - 用户大量请求导致服务不可用

     > 在秒杀和大促开始前,如果准备不充分，用户发起大量请求也会造成服务提供者的不可用

2. 重试请求流量变大

   - 用户大量的重试请求

     > 在服务提供者不可用后，用户由于忍受不了界面上长时间的等待，而不断刷新页面甚至提交表单

   - 代码逻辑重试次数过大

     > 服务调用端的会存在大量服务异常后的重试逻辑

3. 服务调用者不可用

   - 同步等待造成的资源耗尽

     > 当服务调用者使用**同步调用**时，会产生大量的等待线程占用系统资源；一旦线程资源被耗尽，服务调用者提供的服务也将处于不可用状态，于是服务雪崩效应产生了

#### 服务雪崩的解决方案

针对造成服务雪崩的不同原因, 可以使用不同的应对策略:

1. 流量控制

   - 网关限流

     > Nginx + Lua进行网关进行流量控制（OpenResty）

   - 用户交互限流

     > 1. 采用加载动画，提高用户的忍耐等待时间. 
     > 2. 提交按钮添加强制等待时间机制.

   - 关闭重试

2. 改进缓存模式

   - 缓存预加载
   - 同步改为异步刷新

3. 服务自动扩容

4. 服务调用者降级服务

   - 资源隔离：对调用服务的线程池进行隔离

   - 对依赖服务进行分类

     > 根据具体业务，将依赖服务分为：强依赖和弱依赖。
     >
     > - 强依赖服务不可用 **会** 导致当前业务中止
     > - 弱依赖服务的不可用 **不会** 导致当前业务的中止

   - 不可用服务的调用快速失败

     > 不可用服务的调用快速失败的实现方式 
     >
     > - **超时机制** 
     > - **熔断器** 
     > - 熔断后的 **降级方法** 



### 服务熔断

​		服务熔断：当下游的服务（接口提供方）因为某种原因突然**<font color=red>变得不可用</font>**或**<font color=red>响应过慢</font>**，上游服务（调用接口方）为了保证自己整体服务的可用性，<font color=blue>不再继续调用目标服务，直接返回，快速释放资源</font>。如果目标服务情况好转则恢复调用

常用的断路器模式如下：

![服务熔断](.\images\服务熔断1.png)

- 最开始处于`closed`状态，一旦检测到错误到达一定阈值，便转为`open`状态
- 这时候会有个`reset timeout`，到了这个时间了，会转移到`half open`状态
- 尝试放行一部分请求到后端，一旦检测成功便回归到`closed`状态，即恢复服务

上面模式对应的Hystrix配置如下：

````yaml
//滑动窗口的大小，默认为20
circuitBreaker.requestVolumeThreshold 
//过多长时间，熔断器再次检测是否开启，默认为5000，即5s钟
circuitBreaker.sleepWindowInMilliseconds 
//错误率，默认50%
circuitBreaker.errorThresholdPercentage
````

> 针对上面配置的解释：
>
> - 每当20个请求中，有50%失败时，熔断器就会打开，此时再调用此服务，将会直接返回失败，不再调远程服务
> - 直到5s钟之后，重新检测该触发条件，判断是否把熔断器关闭，或者继续打开



### 服务降级

​		服务降级：当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作

​		降级处理时在***客户端***完成的，与服务端没有关系。一般是从 **整体负荷** 考虑，当某个服务熔断之后，服务器将不再被调用，此时客户端可以自己准备一个本地的`FallBack`回调，返回一个缺省值，以此缓解服务器的压力，以保证核心任务的进行。同时保证大部分请求，客户能得到正确的响应





### 服务降级和服务熔断的比较

### 相同之处

1. 目的很一致，都是从可用性可靠性着想，为防止系统的整体缓慢甚至崩溃，采用的技术手段
2. 最终表现类似，对于两者来说，最终让用户体验到的是某些功能暂时不可达或不可用
3. 粒度一般都是服务级别，当然，业界也有不少更细粒度的做法，比如做到数据持久层（允许查询，不允许增删改）
4. 自治性要求很高，熔断模式一般都是服务基于策略的自动触发，降级虽说可人工干预，但在微服务架构下，完全靠人显然不可能，开关预置、配置中心都是必要手段

#### 区别

- 触发原因不太一样，
  - 服务熔断一般是下游服务（接口的提供者）**故障**引起
  - 服务降级一般是从**整体负荷考虑**
- 实现方式不太一样
  - 服务降级具有**代码侵入性(由控制器完成/或自动降级)**
  - 熔断一般称为**自我熔断**
- 管理目标的层次不太一样
  - 熔断其实是一个框架级的处理，每个微服务都需要（无层级之分）
  - 降级一般需要对业务有层级之分（比如降级一般是从最外围服务开始） 